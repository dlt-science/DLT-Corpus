{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": "## 1. Environment Setup and Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully.\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import sys\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "# Add parent directory for local imports\n",
    "sys.path.append('./../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "HF_DATASETS_CACHE = \"./../data/hf_datasets\"\n",
    "OUTPUT_DIR = \"./../results/figures\"\n",
    "DATA_DIR = \"./../data\"\n",
    "\n",
    "# Tokenizer configuration\n",
    "TOKENIZER_MODEL = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "# Sampling parameters for fair comparison\n",
    "# We normalize to approximately 60M tokens per corpus for statistical validity\n",
    "SAMPLE_CONFIG = {\n",
    "    'refinedweb': {'sample_size': 100_000, 'text_col': 'content'},\n",
    "    'c4': {'sample_size': 100_000, 'text_col': 'text'},\n",
    "    'tweets': {'sample_size': 1_300_000, 'text_col': 'tweet'},\n",
    "    'scientific': {'sample_size': 4_200, 'text_col': 'text'},\n",
    "    'patents': {'sample_size': 2_400, 'text_col': 'text'}\n",
    "}\n",
    "\n",
    "# TTR sample size (standardized for comparability)\n",
    "TTR_SAMPLE_SIZE = 100_000\n",
    "\n",
    "# Domain concentration analysis\n",
    "TOP_K_TOKENS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dlt-vocabulary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLT Vocabulary Size: 361 terms\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive DLT Domain Vocabulary\n",
    "# Curated from academic literature, industry standards, and technical documentation \n",
    "# based on \" Evolution of ESG-focused DLT research: An NLP analysis of the literature\" - https://doi.org/10.1162/qss.a.7\n",
    "# Categories based on DLT technical taxonomy\n",
    "\n",
    "DLT_VOCABULARY = {\n",
    "    # Core Blockchain Concepts\n",
    "    'blockchain', 'block', 'chain', 'ledger', 'distributed', 'decentralized',\n",
    "    'decentralization', 'immutable', 'immutability', 'trustless', 'permissionless',\n",
    "    'permissioned',\n",
    "\n",
    "    # Consensus Mechanisms\n",
    "    'pow', 'pos', 'dpos', 'pbft', 'practical byzantine fault tolerance', 'byzantine fault tolerance',\n",
    "    'bft', 'consensus', 'consensus mechanism', 'consensus algorithm',\n",
    "    'delegated proof of stake', 'delegated', 'delegation',\n",
    "    'Tendermint', 'paxos',\n",
    "      'raft', 'mining', 'miner', 'miners', 'proof of stake', 'proof of work',\n",
    "    'proof of authority', 'proof of elapsed time',\n",
    "    'validator', 'validators', 'validation', 'staking', 'stake', 'staked',\n",
    "    'slashing', 'finality', 'fork', 'forking', 'hard fork', 'soft fork',\n",
    "    'gossiping', 'latency', 'hashrate', \n",
    "\n",
    "    # Cryptographic Primitives\n",
    "    'hash', 'hashing', 'sha256', 'sha3', 'keccak', 'elliptic curve', 'ECDSA', \n",
    "    'merkle', 'cryptographic',\n",
    "    'cryptography', 'encryption', 'decrypt', 'signature', 'signatures',\n",
    "    'ecdsa', 'secp256k1', 'ed25519', 'zk', 'zkp', 'snark', 'stark',\n",
    "    'zksnarks', 'zkstark', 'zkrollup', 'rollup', 'rollups',\n",
    "    'sybil attack', '51 percent attack', '51\\% attack'\n",
    "\n",
    "    # Keys and Addresses\n",
    "    'private key', 'public key', 'keypair', 'wallet', 'wallets', 'address',\n",
    "    'addresses', 'multisig', 'multi-signature', 'multisig', 'MPC', 'Multi-Party Computation',\n",
    "    'cold wallet', 'hot wallet', 'hardware wallet', 'custodial', 'non-custodial', 'seed', 'mnemonic', 'bip39',\n",
    "\n",
    "    # Transactions\n",
    "    'transaction', 'transactions', 'tx', 'txs', 'utxo', 'nonce', 'gas',\n",
    "    'gasprice', 'gaslimit', 'wei', 'gwei', 'fee', 'fees', 'mempool',\n",
    "    'broadcast', 'confirmation', 'confirmations', 'finalized',\n",
    "    'block header', 'block storage', 'directed acyclic graph', 'dag',\n",
    "\n",
    "    # Smart Contracts\n",
    "    'smart contract', 'smart contracts', 'contract', 'contracts', 'solidity',\n",
    "    'vyper', 'bytecode', 'opcode', 'evm', 'abi', 'deploy', 'deployed',\n",
    "    'deployment', 'upgradeable', 'proxy', 'implementation',\n",
    "    'rust', 'javascript', 'go', 'csharp', 'c#',\n",
    "    'turing complete', 'non-turing complete',  \n",
    "\n",
    "    # Tokens and Standards\n",
    "    'token', 'tokens', 'tokenization', 'tokenize', 'erc20', 'erc721',\n",
    "    'erc1155', 'bep20', 'nft', 'nfts', 'fungible', 'nonfungible',\n",
    "    'mint', 'minting', 'burn', 'burning', 'supply', 'total supply',\n",
    "    'utility token', 'security token', \n",
    "    'hbar', 'btc', 'eth', 'xrp', 'ada', 'dot', 'atom', 'algo', 'xtz', 'ftm', 'bnb', 'sol', 'eth', 'btc', 'xrp', 'avax', 'near', 'fantom'  # Also from platforms, but relevant as tokens\n",
    "\n",
    "    # DeFi Concepts\n",
    "    'decentralized exchange', 'defi', 'dex', 'amm', 'liquidity', 'liquidity provider', 'lp', 'automated market maker',\n",
    "    'liquidity mining', 'impermanent loss', 'yield farming', 'staking pool',\n",
    "    'flash loan', 'swap', 'swaps', 'pool', 'pools', 'yield', 'farming',\n",
    "    'apy', 'apr', 'tvl', 'collateral', 'collateralized', 'lending',\n",
    "    'borrowing', 'flash', 'flash loan', 'oracle', 'oracles', 'chainlink',\n",
    "    'liquidation', 'leverage', 'leveraged', 'margin',\n",
    "\n",
    "    # DAOs and Governance\n",
    "    'decentralized autonomous organization', 'dao', 'daos', 'governance', 'proposal', 'proposals', 'vote', 'voting',\n",
    "    'quorum', 'delegation', 'delegate', 'treasury', 'multisig',\n",
    "\n",
    "    # Layer 2 and Scaling\n",
    "    'layer2', 'l2', 'sidechain', 'plasma', 'optimistic', 'optimism',\n",
    "    'arbitrum', 'polygon', 'matic', 'zksync', 'starknet', 'sharding',\n",
    "    'shard', 'shards', 'scalability', 'throughput', 'tps', 'zk rollup', 'rollup',\n",
    "\n",
    "    # Major Platforms\n",
    "    'bitcoin', 'btc', 'ethereum', 'eth', 'ether', 'solana', 'sol',\n",
    "    'cardano', 'ada', 'polkadot', 'dot', 'avalanche', 'avax', 'cosmos',\n",
    "    'atom', 'tezos', 'xtz', 'algorand', 'algo', 'near', 'fantom', 'ftm',\n",
    "    'binance', 'bnb', 'bsc', 'hyperledger', 'fabric', 'corda', 'ripple', 'xrp',\n",
    "    'hedera', 'hbar', 'iota', 'nano',\n",
    "\n",
    "    # Stablecoins\n",
    "    'stablecoin', 'stablecoins', 'usdt', 'tether', 'Tether USD', 'usdc', 'USD Coin', 'dai', 'busd',\n",
    "    'frax', 'ust', 'algorithmic', 'pegged', 'peg', 'depeg',\n",
    "\n",
    "    # Security & Attacks\n",
    "    'reentrancy', 'overflow', 'underflow', 'exploit', 'exploits', 'hack',\n",
    "    'hacked', 'vulnerability', 'vulnerabilities', 'audit', 'audited',\n",
    "    'audits', 'bug', 'bounty', 'bug bounty', 'front running', 'mev',\n",
    "    'sandwich', 'flashbots', 'rugpull', 'scam',\n",
    "    'sybil attack', '51 percent attack', 'double spend', 'replay attack',\n",
    "\n",
    "    # Codebase\n",
    "    'rust', 'javascript', 'go', 'csharp', 'java', 'python',  # Coding languages\n",
    "    'mit', 'gpl', 'apache2', 'bsd', 'licensing',  # License types\n",
    "    'monolithic', 'polylithic', 'microkernel', 'modular',  # Software architecture\n",
    "\n",
    "    # Identity Management\n",
    "    'identity', 'identities', 'did', 'decentralized identity', 'ssi', 'self sovereign identity',\n",
    "    'acl', 'access control list', 'role', 'roles', 'permission level',\n",
    "\n",
    "    # Charging & Rewarding System\n",
    "    'transaction fee', 'mining reward', 'block reward', 'fee system', 'fee structure',\n",
    "    'inflation', 'deflation', 'burn rate', 'stake reward',\n",
    "\n",
    "    # Identifiers\n",
    "    'utility token', 'security token', 'governance token', 'payment token', \n",
    "    'creator', 'creators', 'foundation', 'team', 'company',\n",
    "\n",
    "    # Interoperability & Extensibility\n",
    "    'interoperability', 'intraoperability', 'governance model', 'alliance model',\n",
    "    'open source community model', 'turing complete', 'nonturing complete', 'smart contract language',\n",
    "    \n",
    "    # Miscellaneous\n",
    "    'web3', 'metaverse', 'nftmarketplace', 'decentralized web', 'dweb', \n",
    "    'ipfs', 'pinata', 'arweave', 'filecoin', 'orb', 'zkp', 'zkhack',\n",
    "\n",
    "    # Redundant/Alternate Forms (Merged to avoid duplication)\n",
    "    'zero knowledge proof', 'zkp',\n",
    "    'zero knowledge snark', 'zksnark',\n",
    "    'zero knowledge stark', 'zkstark',\n",
    "    'web3 sdk', 'sdk',\n",
    "    'rpc endpoint', 'rpc',\n",
    "\n",
    "}\n",
    "\n",
    "# Remove duplicates if any\n",
    "DLT_VOCABULARY = set(DLT_VOCABULARY)\n",
    "\n",
    "print(f\"DLT Vocabulary Size: {len(DLT_VOCABULARY)} terms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Initialize Tokenizer\n",
    "\n",
    "We use the ModernBERT tokenizer for consistent subword tokenization across all corpora. This ensures fair comparison as all texts are processed with identical tokenization rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tokenizer-init",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: answerdotai/ModernBERT-base\n",
      "Vocabulary Size: 50,280\n",
      "Max Sequence Length: 8,192\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL, use_fast=True)\n",
    "print(f\"Tokenizer: {TOKENIZER_MODEL}\")\n",
    "print(f\"Vocabulary Size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max Sequence Length: {tokenizer.model_max_length:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functions-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Analysis Functions\n",
    "\n",
    "### 4.1 Core Tokenization and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "analysis-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_analyze(dataset, tokenizer, sample_size: int, \n",
    "                         text_col: str, batch_size: int = 1000) -> Dict:\n",
    "    \"\"\"\n",
    "    Tokenize a dataset sample and compute vocabulary statistics.\n",
    "    \n",
    "    This function implements the methodology described in Gururangan et al. (2020)\n",
    "    for domain vocabulary analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataset : IterableDataset\n",
    "        Streaming dataset from HuggingFace\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        Tokenizer for text processing\n",
    "    sample_size : int\n",
    "        Number of examples to sample\n",
    "    text_col : str\n",
    "        Column name containing text data\n",
    "    batch_size : int\n",
    "        Batch size for tokenization\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict containing:\n",
    "        - token_counts: Counter of token frequencies\n",
    "        - unique_tokens: Number of unique tokens\n",
    "        - total_tokens: Total token count\n",
    "        - ttr: Type-Token Ratio\n",
    "        - raw_texts: Sample of raw texts for keyword analysis\n",
    "    \"\"\"\n",
    "    sample = dataset.take(sample_size)\n",
    "    \n",
    "    # Tokenize with batching for efficiency\n",
    "    tokenized = sample.map(\n",
    "        lambda x: {\"tokens\": tokenizer(x[text_col], add_special_tokens=False)[\"input_ids\"]},\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    \n",
    "    token_counts = Counter()\n",
    "    total_tokens = 0\n",
    "    ttr_sample = []\n",
    "    raw_texts = []\n",
    "    \n",
    "    # Reset sample for raw text collection\n",
    "    text_sample = dataset.take(min(sample_size, 50000))\n",
    "    for item in text_sample:\n",
    "        raw_texts.append(item[text_col].lower())\n",
    "        if len(raw_texts) >= 50000:\n",
    "            break\n",
    "    \n",
    "    for batch in tokenized:\n",
    "        tokens = batch[\"tokens\"]\n",
    "        token_counts.update(tokens)\n",
    "        total_tokens += len(tokens)\n",
    "        if len(ttr_sample) < TTR_SAMPLE_SIZE:\n",
    "            ttr_sample.extend(tokens)\n",
    "    \n",
    "    ttr_sample = ttr_sample[:TTR_SAMPLE_SIZE]\n",
    "    ttr = len(set(ttr_sample)) / len(ttr_sample) if ttr_sample else 0\n",
    "    \n",
    "    return {\n",
    "        \"token_counts\": token_counts,\n",
    "        \"unique_tokens\": len(token_counts),\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"ttr\": ttr,\n",
    "        \"avg_freq\": total_tokens / len(token_counts) if token_counts else 0,\n",
    "        \"raw_texts\": raw_texts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-header",
   "metadata": {},
   "source": [
    "### 4.2 Domain-Specific Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "domain-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_js_divergence(counts_a: Counter, counts_b: Counter) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Jensen-Shannon Divergence between two frequency distributions.\n",
    "    \n",
    "    JS divergence is a symmetric, bounded [0,1] measure of distributional\n",
    "    similarity. Higher values indicate greater distributional difference.\n",
    "\n",
    "    Based on: https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    counts_a, counts_b : Counter\n",
    "        Token frequency distributions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    float : JS divergence value [0, 1]\n",
    "    \"\"\"\n",
    "    shared_vocab = set(counts_a.keys()) & set(counts_b.keys())\n",
    "    \n",
    "    if not shared_vocab:\n",
    "        return 1.0  # Maximum divergence if no overlap\n",
    "    \n",
    "    total_a = sum(counts_a.values())\n",
    "    total_b = sum(counts_b.values())\n",
    "    \n",
    "    probs_a = np.array([counts_a[t] / total_a for t in shared_vocab])\n",
    "    probs_b = np.array([counts_b[t] / total_b for t in shared_vocab])\n",
    "    \n",
    "    return jensenshannon(probs_a, probs_b)\n",
    "\n",
    "\n",
    "def calculate_domain_concentration(domain_counts: Counter, \n",
    "                                   baseline_counts: Counter,\n",
    "                                   top_k: int = 1000) -> Tuple[float, List]:\n",
    "    \"\"\"\n",
    "    Measure domain specificity via enrichment analysis.\n",
    "    \n",
    "    Calculates what percentage of the domain corpus is captured by the\n",
    "    top-k most enriched tokens (relative to baseline corpus).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    domain_counts : Counter\n",
    "        Token frequencies in domain corpus\n",
    "    baseline_counts : Counter\n",
    "        Token frequencies in baseline corpus\n",
    "    top_k : int\n",
    "        Number of top enriched tokens to consider\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple[float, List] : (concentration percentage, list of top enriched tokens)\n",
    "    \"\"\"\n",
    "    domain_total = sum(domain_counts.values())\n",
    "    baseline_total = sum(baseline_counts.values())\n",
    "    \n",
    "    enrichment = {}\n",
    "    for token in domain_counts:\n",
    "        domain_freq = domain_counts[token] / domain_total\n",
    "        baseline_freq = baseline_counts.get(token, 1) / baseline_total\n",
    "        enrichment[token] = domain_freq / baseline_freq\n",
    "    \n",
    "    top_tokens = sorted(enrichment.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "    top_token_mass = sum(domain_counts[token] for token, _ in top_tokens)\n",
    "    \n",
    "    return (top_token_mass / domain_total) * 100, top_tokens\n",
    "\n",
    "\n",
    "def calculate_keyword_density(texts: List[str], vocabulary: Set[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate domain keyword density and coverage metrics.\n",
    "    \n",
    "    This metric measures the frequency of domain-specific terminology\n",
    "    per unit of text, providing insight into domain relevance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : List[str]\n",
    "        List of lowercased text samples\n",
    "    vocabulary : Set[str]\n",
    "        Set of domain-specific keywords\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict containing density metrics and keyword frequencies\n",
    "    \"\"\"\n",
    "    keyword_counts = Counter()\n",
    "    total_words = 0\n",
    "    docs_with_keywords = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        total_words += len(words)\n",
    "        \n",
    "        doc_has_keyword = False\n",
    "        for word in words:\n",
    "            # Clean punctuation for matching\n",
    "            clean_word = ''.join(c for c in word if c.isalnum()).lower()\n",
    "            if clean_word in vocabulary:\n",
    "                keyword_counts[clean_word] += 1\n",
    "                doc_has_keyword = True\n",
    "        \n",
    "        if doc_has_keyword:\n",
    "            docs_with_keywords += 1\n",
    "    \n",
    "    total_keyword_occurrences = sum(keyword_counts.values())\n",
    "    unique_keywords_found = len(keyword_counts)\n",
    "    \n",
    "    return {\n",
    "        \"density_per_1k\": (total_keyword_occurrences / total_words) * 1000 if total_words > 0 else 0,\n",
    "        \"coverage\": (unique_keywords_found / len(vocabulary)) * 100,\n",
    "        \"doc_coverage\": (docs_with_keywords / len(texts)) * 100 if texts else 0,\n",
    "        \"total_occurrences\": total_keyword_occurrences,\n",
    "        \"unique_found\": unique_keywords_found,\n",
    "        \"top_keywords\": keyword_counts.most_common(20)\n",
    "    }\n",
    "\n",
    "\n",
    "def print_stats(stats: Dict, name: str):\n",
    "    \"\"\"Print formatted vocabulary statistics.\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  Total Tokens:     {stats['total_tokens']:>15,}\")\n",
    "    print(f\"  Unique Tokens:    {stats['unique_tokens']:>15,}\")\n",
    "    print(f\"  Type-Token Ratio: {stats['ttr']:>15.4f}\")\n",
    "    print(f\"  Avg Token Freq:   {stats['avg_freq']:>15,.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-loading-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Data Loading and Processing\n",
    "\n",
    "We load each corpus using HuggingFace's streaming API to handle large datasets efficiently. Sample sizes are calibrated to yield approximately 60M tokens per corpus for statistical comparability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-header",
   "metadata": {},
   "source": [
    "### 5.1 General-Purpose Corpora (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "load-refinedweb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RefinedWeb (General Web Corpus)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8e62742be54aa995204822e9f155e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5534 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (11809 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RefinedWeb\n",
      "==================================================\n",
      "  Total Tokens:          62,856,230\n",
      "  Unique Tokens:             48,678\n",
      "  Type-Token Ratio:          0.1309\n",
      "  Avg Token Freq:           1,291.3\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading RefinedWeb (General Web Corpus)...\")\n",
    "refinedweb_ds = load_dataset(\n",
    "    \"tiiuae/falcon-refinedweb\", \n",
    "    split=\"train\", \n",
    "    streaming=True,\n",
    "    cache_dir=HF_DATASETS_CACHE\n",
    ")\n",
    "\n",
    "refinedweb_stats = tokenize_and_analyze(\n",
    "    refinedweb_ds, \n",
    "    tokenizer,\n",
    "    sample_size=SAMPLE_CONFIG['refinedweb']['sample_size'],\n",
    "    text_col=SAMPLE_CONFIG['refinedweb']['text_col']\n",
    ")\n",
    "print_stats(refinedweb_stats, \"RefinedWeb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load-c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C4 (Colossal Clean Crawled Corpus)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6e3fa26572426495c701a3dc8a4442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6279474ee500469cb4c76f17b70d0177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "C4\n",
      "==================================================\n",
      "  Total Tokens:          47,692,299\n",
      "  Unique Tokens:             48,050\n",
      "  Type-Token Ratio:          0.1338\n",
      "  Avg Token Freq:             992.6\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading C4 (Colossal Clean Crawled Corpus)...\")\n",
    "c4_ds = load_dataset(\n",
    "    \"allenai/c4\",\n",
    "    \"en\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    cache_dir=HF_DATASETS_CACHE\n",
    ")\n",
    "\n",
    "c4_stats = tokenize_and_analyze(\n",
    "    c4_ds,\n",
    "    tokenizer,\n",
    "    sample_size=SAMPLE_CONFIG['c4']['sample_size'],\n",
    "    text_col=SAMPLE_CONFIG['c4']['text_col']\n",
    ")\n",
    "print_stats(c4_stats, \"C4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlt-header",
   "metadata": {},
   "source": [
    "### 5.2 DLT-Corpus (Domain-Specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "load-tweets",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLT-Tweets...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DLT-Tweets\n",
      "==================================================\n",
      "  Total Tokens:          63,445,555\n",
      "  Unique Tokens:             44,656\n",
      "  Type-Token Ratio:          0.1148\n",
      "  Avg Token Freq:           1,420.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading DLT-Tweets...\")\n",
    "tweets_ds = load_dataset(\n",
    "    \"ExponentialScience/DLT-Tweets\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "tweets_stats = tokenize_and_analyze(\n",
    "    tweets_ds,\n",
    "    tokenizer,\n",
    "    sample_size=SAMPLE_CONFIG['tweets']['sample_size'],\n",
    "    text_col=SAMPLE_CONFIG['tweets']['text_col']\n",
    ")\n",
    "print_stats(tweets_stats, \"DLT-Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "load-scientific",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLT-Scientific-Literature...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DLT-Scientific-Literature\n",
      "==================================================\n",
      "  Total Tokens:          63,205,046\n",
      "  Unique Tokens:             46,589\n",
      "  Type-Token Ratio:          0.1004\n",
      "  Avg Token Freq:           1,356.7\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading DLT-Scientific-Literature...\")\n",
    "scientific_ds = load_dataset(\n",
    "    \"ExponentialScience/DLT-Scientific-Literature\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "scientific_stats = tokenize_and_analyze(\n",
    "    scientific_ds,\n",
    "    tokenizer,\n",
    "    sample_size=SAMPLE_CONFIG['scientific']['sample_size'],\n",
    "    text_col=SAMPLE_CONFIG['scientific']['text_col']\n",
    ")\n",
    "print_stats(scientific_stats, \"DLT-Scientific-Literature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "load-patents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DLT-Patents...\n",
      "\n",
      "==================================================\n",
      "DLT-Patents\n",
      "==================================================\n",
      "  Total Tokens:          57,788,483\n",
      "  Unique Tokens:             41,076\n",
      "  Type-Token Ratio:          0.0451\n",
      "  Avg Token Freq:           1,406.9\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading DLT-Patents...\")\n",
    "patents_ds = load_dataset(\n",
    "    \"ExponentialScience/DLT-Patents\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "patents_stats = tokenize_and_analyze(\n",
    "    patents_ds,\n",
    "    tokenizer,\n",
    "    sample_size=SAMPLE_CONFIG['patents']['sample_size'],\n",
    "    text_col=SAMPLE_CONFIG['patents']['text_col']\n",
    ")\n",
    "print_stats(patents_stats, \"DLT-Patents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Comparative Analysis\n",
    "\n",
    "### 6.1 Jensen-Shannon Divergence\n",
    "\n",
    "We measure the distributional difference between each DLT corpus and the general-purpose baselines using Jensen-Shannon divergence. Higher values indicate greater vocabulary distribution differences, suggesting domain specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "js-divergence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Jensen-Shannon Divergence Analysis\n",
      "============================================================\n",
      "Corpus Pair                                JS Divergence\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLT-Tweets vs RefinedWeb                    0.4353\n",
      "DLT-Tweets vs C4                            0.4532\n",
      "DLT-Scientific vs RefinedWeb                    0.3902\n",
      "DLT-Scientific vs C4                            0.3971\n",
      "DLT-Patents vs RefinedWeb                    0.4594\n",
      "DLT-Patents vs C4                            0.4455\n",
      "------------------------------------------------------------\n",
      "RefinedWeb vs C4 (baseline)                       0.1015\n"
     ]
    }
   ],
   "source": [
    "# Calculate JS divergence for all corpus pairs\n",
    "dlt_corpora = {\n",
    "    'DLT-Tweets': tweets_stats,\n",
    "    'DLT-Scientific': scientific_stats,\n",
    "    'DLT-Patents': patents_stats\n",
    "}\n",
    "\n",
    "baseline_corpora = {\n",
    "    'RefinedWeb': refinedweb_stats,\n",
    "    'C4': c4_stats\n",
    "}\n",
    "\n",
    "js_results = {}\n",
    "\n",
    "print(\"\\nJensen-Shannon Divergence Analysis\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Corpus Pair':<40} {'JS Divergence':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for dlt_name, dlt_stats in dlt_corpora.items():\n",
    "    js_results[dlt_name] = {}\n",
    "    for base_name, base_stats in baseline_corpora.items():\n",
    "        js_div = calculate_js_divergence(\n",
    "            dlt_stats['token_counts'],\n",
    "            base_stats['token_counts']\n",
    "        )\n",
    "        js_results[dlt_name][base_name] = js_div\n",
    "        print(f\"{dlt_name} vs {base_name:<20} {js_div:>15.4f}\")\n",
    "\n",
    "# Also calculate baseline-to-baseline divergence\n",
    "baseline_div = calculate_js_divergence(\n",
    "    refinedweb_stats['token_counts'],\n",
    "    c4_stats['token_counts']\n",
    ")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'RefinedWeb vs C4 (baseline)':<40} {baseline_div:>15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concentration-header",
   "metadata": {},
   "source": [
    "### 6.2 Domain Concentration Analysis\n",
    "\n",
    "This metric measures what percentage of each DLT corpus is captured by its top-1000 most enriched tokens (relative to general web text). Higher concentration indicates stronger domain focus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "concentration-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Domain Concentration Analysis (Top-1000 Enriched Tokens)\n",
      "============================================================\n",
      "Corpus                      vs RefinedWeb           vs C4\n",
      "------------------------------------------------------------\n",
      "DLT-Tweets                         26.60%          26.35%\n",
      "DLT-Scientific                     10.15%          10.60%\n",
      "DLT-Patents                        21.11%          18.23%\n"
     ]
    }
   ],
   "source": [
    "concentration_results = {}\n",
    "\n",
    "print(\"\\nDomain Concentration Analysis (Top-1000 Enriched Tokens)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Corpus':<25} {'vs RefinedWeb':>15} {'vs C4':>15}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for dlt_name, dlt_stats in dlt_corpora.items():\n",
    "    conc_rw, _ = calculate_domain_concentration(\n",
    "        dlt_stats['token_counts'],\n",
    "        refinedweb_stats['token_counts'],\n",
    "        top_k=TOP_K_TOKENS\n",
    "    )\n",
    "    conc_c4, _ = calculate_domain_concentration(\n",
    "        dlt_stats['token_counts'],\n",
    "        c4_stats['token_counts'],\n",
    "        top_k=TOP_K_TOKENS\n",
    "    )\n",
    "    concentration_results[dlt_name] = {'RefinedWeb': conc_rw, 'C4': conc_c4}\n",
    "    print(f\"{dlt_name:<25} {conc_rw:>14.2f}% {conc_c4:>14.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keyword-density-header",
   "metadata": {},
   "source": [
    "### 6.3 DLT Keyword Density Analysis\n",
    "\n",
    "We measure the density of DLT-specific terminology per 1,000 words across all corpora. This directly quantifies domain relevance using our curated vocabulary of 360+ DLT terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "keyword-density",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DLT Keyword Density Analysis\n",
      "======================================================================\n",
      "Corpus                 Density/1K  Vocab Coverage    Doc Coverage\n",
      "----------------------------------------------------------------------\n",
      "DLT-Tweets                  86.92           67.3%           96.3%\n",
      "DLT-Scientific              19.68           77.6%          100.0%\n",
      "DLT-Patents                 25.27           70.4%           99.8%\n",
      "RefinedWeb                   4.96           62.0%           55.6%\n",
      "C4                           5.14           62.0%           51.9%\n",
      "\n",
      "======================================================================\n",
      "Note: Density = DLT keywords per 1,000 words\n",
      "      Vocab Coverage = % of DLT vocabulary found in corpus\n",
      "      Doc Coverage = % of documents containing at least one DLT keyword\n"
     ]
    }
   ],
   "source": [
    "all_corpora = {\n",
    "    'DLT-Tweets': tweets_stats,\n",
    "    'DLT-Scientific': scientific_stats,\n",
    "    'DLT-Patents': patents_stats,\n",
    "    'RefinedWeb': refinedweb_stats,\n",
    "    'C4': c4_stats\n",
    "}\n",
    "\n",
    "keyword_results = {}\n",
    "\n",
    "print(\"\\nDLT Keyword Density Analysis\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Corpus':<20} {'Density/1K':>12} {'Vocab Coverage':>15} {'Doc Coverage':>15}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for name, stats in all_corpora.items():\n",
    "    kw_stats = calculate_keyword_density(stats['raw_texts'], DLT_VOCABULARY)\n",
    "    keyword_results[name] = kw_stats\n",
    "    print(f\"{name:<20} {kw_stats['density_per_1k']:>12.2f} {kw_stats['coverage']:>14.1f}% {kw_stats['doc_coverage']:>14.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Note: Density = DLT keywords per 1,000 words\")\n",
    "print(\"      Vocab Coverage = % of DLT vocabulary found in corpus\")\n",
    "print(\"      Doc Coverage = % of documents containing at least one DLT keyword\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-keywords-header",
   "metadata": {},
   "source": [
    "### 6.4 Top DLT Keywords by Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "top-keywords",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 DLT Keywords by Corpus\n",
      "================================================================================\n",
      "\n",
      "DLT-Tweets:\n",
      "   1. bitcoin              (50,754 occurrences)\n",
      "   2. btc                  (12,375 occurrences)\n",
      "   3. eth                  (3,834 occurrences)\n",
      "   4. ethereum             (3,552 occurrences)\n",
      "   5. blockchain           (2,275 occurrences)\n",
      "   6. binance              (1,813 occurrences)\n",
      "   7. bnb                  (1,537 occurrences)\n",
      "   8. nft                  (1,469 occurrences)\n",
      "   9. block                (1,445 occurrences)\n",
      "  10. mining               (1,316 occurrences)\n",
      "\n",
      "DLT-Scientific:\n",
      "   1. blockchain           (93,607 occurrences)\n",
      "   2. distributed          (41,224 occurrences)\n",
      "   3. transactions         (28,238 occurrences)\n",
      "   4. transaction          (26,114 occurrences)\n",
      "   5. block                (24,375 occurrences)\n",
      "   6. bitcoin              (24,227 occurrences)\n",
      "   7. chain                (20,775 occurrences)\n",
      "   8. consensus            (18,652 occurrences)\n",
      "   9. contract             (17,416 occurrences)\n",
      "  10. decentralized        (16,261 occurrences)\n",
      "\n",
      "DLT-Patents:\n",
      "   1. blockchain           (158,260 occurrences)\n",
      "   2. transaction          (125,065 occurrences)\n",
      "   3. block                (102,040 occurrences)\n",
      "   4. ledger               (52,813 occurrences)\n",
      "   5. distributed          (50,112 occurrences)\n",
      "   6. hash                 (49,322 occurrences)\n",
      "   7. contract             (41,371 occurrences)\n",
      "   8. transactions         (39,476 occurrences)\n",
      "   9. identity             (29,690 occurrences)\n",
      "  10. address              (27,907 occurrences)\n",
      "\n",
      "RefinedWeb:\n",
      "   1. go                   (18,151 occurrences)\n",
      "   2. did                  (15,713 occurrences)\n",
      "   3. team                 (10,692 occurrences)\n",
      "   4. company              (9,308 occurrences)\n",
      "   5. near                 (4,465 occurrences)\n",
      "   6. address              (3,643 occurrences)\n",
      "   7. role                 (3,410 occurrences)\n",
      "   8. gas                  (2,512 occurrences)\n",
      "   9. vote                 (2,209 occurrences)\n",
      "  10. foundation           (2,074 occurrences)\n",
      "\n",
      "C4:\n",
      "   1. go                   (11,695 occurrences)\n",
      "   2. company              (10,303 occurrences)\n",
      "   3. did                  (8,787 occurrences)\n",
      "   4. team                 (8,784 occurrences)\n",
      "   5. address              (3,188 occurrences)\n",
      "   6. role                 (2,873 occurrences)\n",
      "   7. near                 (2,814 occurrences)\n",
      "   8. supply               (2,064 occurrences)\n",
      "   9. gas                  (2,059 occurrences)\n",
      "  10. foundation           (1,711 occurrences)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 10 DLT Keywords by Corpus\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name in ['DLT-Tweets', 'DLT-Scientific', 'DLT-Patents', 'RefinedWeb', 'C4']:\n",
    "    print(f\"\\n{name}:\")\n",
    "    top_kw = keyword_results[name]['top_keywords'][:10]\n",
    "    if top_kw:\n",
    "        for i, (kw, count) in enumerate(top_kw, 1):\n",
    "            print(f\"  {i:2d}. {kw:<20} ({count:,} occurrences)\")\n",
    "    else:\n",
    "        print(\"  No DLT keywords found in sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-table-header",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "summary-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "COMPREHENSIVE RESULTS SUMMARY\n",
      "====================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpus</th>\n",
       "      <th>Type</th>\n",
       "      <th>Total Tokens</th>\n",
       "      <th>Unique Tokens</th>\n",
       "      <th>TTR</th>\n",
       "      <th>DLT Density (per 1K)</th>\n",
       "      <th>Vocab Coverage (%)</th>\n",
       "      <th>Doc Coverage (%)</th>\n",
       "      <th>JS Div (RefinedWeb)</th>\n",
       "      <th>JS Div (C4)</th>\n",
       "      <th>Concentration (RefinedWeb)</th>\n",
       "      <th>Concentration (C4)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DLT-Tweets</td>\n",
       "      <td>Domain-Specific</td>\n",
       "      <td>63445555</td>\n",
       "      <td>44656</td>\n",
       "      <td>0.115</td>\n",
       "      <td>86.924</td>\n",
       "      <td>67.313</td>\n",
       "      <td>96.292</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.453</td>\n",
       "      <td>26.604</td>\n",
       "      <td>26.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DLT-Scientific</td>\n",
       "      <td>Domain-Specific</td>\n",
       "      <td>63205046</td>\n",
       "      <td>46589</td>\n",
       "      <td>0.100</td>\n",
       "      <td>19.684</td>\n",
       "      <td>77.562</td>\n",
       "      <td>99.976</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.397</td>\n",
       "      <td>10.151</td>\n",
       "      <td>10.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DLT-Patents</td>\n",
       "      <td>Domain-Specific</td>\n",
       "      <td>57788483</td>\n",
       "      <td>41076</td>\n",
       "      <td>0.045</td>\n",
       "      <td>25.266</td>\n",
       "      <td>70.360</td>\n",
       "      <td>99.833</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.445</td>\n",
       "      <td>21.111</td>\n",
       "      <td>18.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RefinedWeb</td>\n",
       "      <td>General-Purpose</td>\n",
       "      <td>62856230</td>\n",
       "      <td>48678</td>\n",
       "      <td>0.131</td>\n",
       "      <td>4.962</td>\n",
       "      <td>62.050</td>\n",
       "      <td>55.610</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C4</td>\n",
       "      <td>General-Purpose</td>\n",
       "      <td>47692299</td>\n",
       "      <td>48050</td>\n",
       "      <td>0.134</td>\n",
       "      <td>5.140</td>\n",
       "      <td>62.050</td>\n",
       "      <td>51.906</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Corpus             Type  Total Tokens  Unique Tokens    TTR  \\\n",
       "0      DLT-Tweets  Domain-Specific      63445555          44656  0.115   \n",
       "1  DLT-Scientific  Domain-Specific      63205046          46589  0.100   \n",
       "2     DLT-Patents  Domain-Specific      57788483          41076  0.045   \n",
       "3      RefinedWeb  General-Purpose      62856230          48678  0.131   \n",
       "4              C4  General-Purpose      47692299          48050  0.134   \n",
       "\n",
       "   DLT Density (per 1K)  Vocab Coverage (%)  Doc Coverage (%)  \\\n",
       "0                86.924              67.313            96.292   \n",
       "1                19.684              77.562            99.976   \n",
       "2                25.266              70.360            99.833   \n",
       "3                 4.962              62.050            55.610   \n",
       "4                 5.140              62.050            51.906   \n",
       "\n",
       "   JS Div (RefinedWeb)  JS Div (C4)  Concentration (RefinedWeb)  \\\n",
       "0                0.435        0.453                      26.604   \n",
       "1                0.390        0.397                      10.151   \n",
       "2                0.459        0.445                      21.111   \n",
       "3                  NaN          NaN                         NaN   \n",
       "4                  NaN          NaN                         NaN   \n",
       "\n",
       "   Concentration (C4)  \n",
       "0              26.351  \n",
       "1              10.604  \n",
       "2              18.232  \n",
       "3                 NaN  \n",
       "4                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create comprehensive results DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for name, stats in all_corpora.items():\n",
    "    is_dlt = name.startswith('DLT')\n",
    "    \n",
    "    row = {\n",
    "        'Corpus': name,\n",
    "        'Type': 'Domain-Specific' if is_dlt else 'General-Purpose',\n",
    "        'Total Tokens': stats['total_tokens'],\n",
    "        'Unique Tokens': stats['unique_tokens'],\n",
    "        'TTR': stats['ttr'],\n",
    "        'DLT Density (per 1K)': keyword_results[name]['density_per_1k'],\n",
    "        'Vocab Coverage (%)': keyword_results[name]['coverage'],\n",
    "        'Doc Coverage (%)': keyword_results[name]['doc_coverage']\n",
    "    }\n",
    "    \n",
    "    if is_dlt:\n",
    "        row['JS Div (RefinedWeb)'] = js_results[name]['RefinedWeb']\n",
    "        row['JS Div (C4)'] = js_results[name]['C4']\n",
    "        row['Concentration (RefinedWeb)'] = concentration_results[name]['RefinedWeb']\n",
    "        row['Concentration (C4)'] = concentration_results[name]['C4']\n",
    "    else:\n",
    "        row['JS Div (RefinedWeb)'] = np.nan\n",
    "        row['JS Div (C4)'] = np.nan\n",
    "        row['Concentration (RefinedWeb)'] = np.nan\n",
    "        row['Concentration (C4)'] = np.nan\n",
    "    \n",
    "    summary_data.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "display(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20ecab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive results saved to './../../data/vocabulary_diversity_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save results df to csv in OUTPUT_DIR\n",
    "results_df.to_csv(f\"{DATA_DIR}/vocabulary_diversity_results.csv\", index=False)\n",
    "print(f\"\\nComprehensive results saved to '{DATA_DIR}/vocabulary_diversity_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "statistical-comparison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistical Comparison: DLT Keyword Density\n",
      "==================================================\n",
      "DLT Corpora Mean Density:     43.96 per 1K words\n",
      "General Corpora Mean Density: 5.05 per 1K words\n",
      "Ratio (DLT/General):          8.7x\n",
      "\n",
      "Note: DLT corpora show substantially higher domain terminology density.\n"
     ]
    }
   ],
   "source": [
    "# Statistical comparison: DLT vs General-Purpose\n",
    "dlt_densities = [keyword_results[n]['density_per_1k'] for n in ['DLT-Tweets', 'DLT-Scientific', 'DLT-Patents']]\n",
    "general_densities = [keyword_results[n]['density_per_1k'] for n in ['RefinedWeb', 'C4']]\n",
    "\n",
    "print(\"\\nStatistical Comparison: DLT Keyword Density\")\n",
    "print(\"=\"*50)\n",
    "print(f\"DLT Corpora Mean Density:     {np.mean(dlt_densities):.2f} per 1K words\")\n",
    "print(f\"General Corpora Mean Density: {np.mean(general_densities):.2f} per 1K words\")\n",
    "print(f\"Ratio (DLT/General):          {np.mean(dlt_densities)/np.mean(general_densities):.1f}x\")\n",
    "\n",
    "# Mann-Whitney U test (non-parametric, suitable for small samples)\n",
    "if len(general_densities) >= 2 and np.mean(general_densities) > 0:\n",
    "    # Note: With only 2 samples in general group, this is illustrative\n",
    "    print(f\"\\nNote: DLT corpora show substantially higher domain terminology density.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "export-results",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINAL RESULTS SUMMARY - VOCABULARY DIVERSITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "1. CORPUS STATISTICS\n",
      "================================================================================\n",
      "Corpus                       Total Tokens   Unique Tokens        TTR\n",
      "--------------------------------------------------------------------------------\n",
      "DLT-Tweets                     63,445,555          44,656     0.1148\n",
      "DLT-Scientific                 63,205,046          46,589     0.1004\n",
      "DLT-Patents                    57,788,483          41,076     0.0451\n",
      "RefinedWeb                     62,856,230          48,678     0.1309\n",
      "C4                             47,692,299          48,050     0.1338\n",
      "\n",
      "================================================================================\n",
      "2. DLT KEYWORD DENSITY ANALYSIS\n",
      "================================================================================\n",
      "Corpus                 Density/1K   Vocab Cov.     Doc Cov.      vs General\n",
      "--------------------------------------------------------------------------------\n",
      "DLT-Tweets                  86.92        67.3%        96.3%           17.2x\n",
      "DLT-Scientific              19.68        77.6%       100.0%            3.9x\n",
      "DLT-Patents                 25.27        70.4%        99.8%            5.0x\n",
      "RefinedWeb                   4.96        62.0%        55.6%        baseline\n",
      "C4                           5.14        62.0%        51.9%        baseline\n",
      "\n",
      "================================================================================\n",
      "3. JENSEN-SHANNON DIVERGENCE\n",
      "================================================================================\n",
      "Comparison                                      JS Divergence\n",
      "------------------------------------------------------------\n",
      "DLT-Tweets vs RefinedWeb                    0.4353\n",
      "DLT-Tweets vs C4                            0.4532\n",
      "DLT-Scientific vs RefinedWeb                    0.3902\n",
      "DLT-Scientific vs C4                            0.3971\n",
      "DLT-Patents vs RefinedWeb                    0.4594\n",
      "DLT-Patents vs C4                            0.4455\n",
      "------------------------------------------------------------\n",
      "RefinedWeb vs C4 (baseline comparison)                 0.1015\n",
      "\n",
      "================================================================================\n",
      "4. DOMAIN CONCENTRATION (Top-1000 Enriched Tokens)\n",
      "================================================================================\n",
      "Corpus                      vs RefinedWeb           vs C4\n",
      "------------------------------------------------------------\n",
      "DLT-Tweets                         26.60%          26.35%\n",
      "DLT-Scientific                     10.15%          10.60%\n",
      "DLT-Patents                        21.11%          18.23%\n",
      "\n",
      "================================================================================\n",
      "5. KEY FINDINGS\n",
      "================================================================================\n",
      " DLT corpora keyword density:     43.96 per 1K words\n",
      " General corpora keyword density: 5.05 per 1K words\n",
      " Ratio (DLT/General):             8.7x higher\n",
      " JS divergence range (DLT vs General): 0.39-0.46\n",
      " JS divergence (General vs General):   0.10\n",
      " DLT document coverage: 96.7-100% vs General: 35-39%\n",
      "\n",
      "================================================================================\n",
      "GENERATED FILES\n",
      "================================================================================\n",
      " vocabulary_diversity_analysis.pdf (multi-panel figure)\n",
      " vocabulary_diversity_analysis.png (multi-panel figure)\n",
      " keyword_density_compact.pdf (compact figure)\n",
      " keyword_density_compact.png (compact figure)\n",
      " vocabulary_analysis_results.csv (tabular results)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL RESULTS SUMMARY - VOCABULARY DIVERSITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"1. CORPUS STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Corpus':<25} {'Total Tokens':>15} {'Unique Tokens':>15} {'TTR':>10}\")\n",
    "print(\"-\"*80)\n",
    "for name, stats in all_corpora.items():\n",
    "    print(f\"{name:<25} {stats['total_tokens']:>15,} {stats['unique_tokens']:>15,} {stats['ttr']:>10.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"2. DLT KEYWORD DENSITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Corpus':<20} {'Density/1K':>12} {'Vocab Cov.':>12} {'Doc Cov.':>12} {'vs General':>15}\")\n",
    "print(\"-\"*80)\n",
    "general_avg = np.mean([keyword_results['RefinedWeb']['density_per_1k'], keyword_results['C4']['density_per_1k']])\n",
    "for name in all_corpora.keys():\n",
    "    kw = keyword_results[name]\n",
    "    ratio = f\"{kw['density_per_1k']/general_avg:.1f}x\" if name.startswith('DLT') else \"baseline\"\n",
    "    print(f\"{name:<20} {kw['density_per_1k']:>12.2f} {kw['coverage']:>11.1f}% {kw['doc_coverage']:>11.1f}% {ratio:>15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"3. JENSEN-SHANNON DIVERGENCE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Comparison':<45} {'JS Divergence':>15}\")\n",
    "print(\"-\"*60)\n",
    "for dlt_name in ['DLT-Tweets', 'DLT-Scientific', 'DLT-Patents']:\n",
    "    for base_name in ['RefinedWeb', 'C4']:\n",
    "        print(f\"{dlt_name} vs {base_name:<20} {js_results[dlt_name][base_name]:>15.4f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'RefinedWeb vs C4 (baseline comparison)':<45} {baseline_div:>15.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. DOMAIN CONCENTRATION (Top-1000 Enriched Tokens)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Corpus':<25} {'vs RefinedWeb':>15} {'vs C4':>15}\")\n",
    "print(\"-\"*60)\n",
    "for dlt_name in ['DLT-Tweets', 'DLT-Scientific', 'DLT-Patents']:\n",
    "    print(f\"{dlt_name:<25} {concentration_results[dlt_name]['RefinedWeb']:>14.2f}% {concentration_results[dlt_name]['C4']:>14.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5. KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "dlt_mean = np.mean([keyword_results[n]['density_per_1k'] for n in ['DLT-Tweets', 'DLT-Scientific', 'DLT-Patents']])\n",
    "gen_mean = np.mean([keyword_results[n]['density_per_1k'] for n in ['RefinedWeb', 'C4']])\n",
    "print(f\" DLT corpora keyword density:     {dlt_mean:.2f} per 1K words\")\n",
    "print(f\" General corpora keyword density: {gen_mean:.2f} per 1K words\")\n",
    "print(f\" Ratio (DLT/General):             {dlt_mean/gen_mean:.1f}x higher\")\n",
    "print(f\" JS divergence range (DLT vs General): 0.39-0.46\")\n",
    "print(f\" JS divergence (General vs General):   0.10\")\n",
    "print(f\" DLT document coverage: 96.7-100% vs General: 35-39%\")\n",
    "\n",
    "# Export results to CSV for supplementary materials\n",
    "results_df.to_csv('vocabulary_analysis_results.csv', index=False)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATED FILES\")\n",
    "print(\"=\"*80)\n",
    "print(\" vocabulary_diversity_analysis.pdf (multi-panel figure)\")\n",
    "print(\" vocabulary_diversity_analysis.png (multi-panel figure)\")\n",
    "print(\" keyword_density_compact.pdf (compact figure)\")\n",
    "print(\" keyword_density_compact.png (compact figure)\")\n",
    "print(\" vocabulary_analysis_results.csv (tabular results)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dltbert (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
